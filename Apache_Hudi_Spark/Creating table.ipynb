{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4d3ef0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import os\n",
    "import pyspark\n",
    "import sys\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a79affc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/anaconda3/envs/spark_hudi/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/anshumanr/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/anshumanr/.ivy2/jars\n",
      "org.apache.hudi#hudi-spark3.3-bundle_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-3730f5f0-32d1-4fae-95ca-d75af4cc64da;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hudi#hudi-spark3.3-bundle_2.12;0.13.1 in central\n",
      ":: resolution report :: resolve 247ms :: artifacts dl 11ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.hudi#hudi-spark3.3-bundle_2.12;0.13.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-3730f5f0-32d1-4fae-95ca-d75af4cc64da\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 1 already retrieved (0kB/11ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/31 16:04:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/31 16:04:05 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "Spark Running\n"
     ]
    }
   ],
   "source": [
    "## Start Spark Session\n",
    "spark = SparkSession.builder.appName('app_name') \\\n",
    "        .config('spark.jars.packages','org.apache.hudi:hudi-spark3.3-bundle_2.12:0.13.1') \\\n",
    "        .config('spark.serializer','org.apache.spark.serializer.KryoSerializer') \\\n",
    "        .config('spark.sql.extensions','org.apache.spark.sql.hudi.HoodieSparkSessionExtension') \\\n",
    "        .config('spark.sql.catalog.spark_catalog','org.apache.spark.sql.hudi.catalog.HoodieCatalog') \\\n",
    "        .getOrCreate()\n",
    "print(\"Spark Running\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f5f8ce-3354-41b8-8b79-4a59ce8f8b8f",
   "metadata": {},
   "source": [
    "## Creating Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fdf3993-79c9-4995-b041-b1595749afcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tablename='Table1'\n",
    "basePath = \"/Users/anshumanr/Documents/Hudi/Apache_Hudi_Spark/My_warehouse/Table1\"\n",
    "dataGen = spark._jvm.org.apache.hudi.QuickstartUtils.DataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74b605ba-cf88-4fa2-bc31-364e9fb01d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "inserts = spark._jvm.org.apache.hudi.QuickstartUtils.convertToStringList(dataGen.generateInserts(10))\n",
    "df = spark.read.json(spark.sparkContext.parallelize(inserts, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "868d5c8b-5b7d-45e6-9e64-d9541efb7b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "hudi_options = {\n",
    "    'hoodie.table.name': tablename,\n",
    "    'hoodie.datasource.write.recordkey.field': 'uuid',\n",
    "    'hoodie.datasource.write.table.name': tablename,\n",
    "    'hoodie.datasource.write.operation': 'upsert',\n",
    "    'hoodie.datasource.write.precombine.field': 'ts'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcb6a1e8-24ad-48ad-8ede-a36eab9d54e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/31 16:04:21 WARN DFSPropertiesConfiguration: Cannot find HUDI_CONF_DIR, please set it as the dir of hudi-defaults.conf\n",
      "23/08/31 16:04:21 WARN DFSPropertiesConfiguration: Properties file file:/etc/hudi/conf/hudi-defaults.conf not found. Ignoring to load props file\n",
      "23/08/31 16:04:22 WARN HoodieBackedTableMetadata: Metadata table was not found at path /Users/anshumanr/Documents/Hudi/Apache_Hudi_Spark/My_warehouse/Table1/.hoodie/metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# WARNING: Unable to get Instrumentation. Dynamic Attach failed. You may add this JAR as -javaagent manually, or supply -Djdk.attach.allowAttachSelf\n",
      "# WARNING: Unable to attach Serviceability Agent. Unable to attach even with module exceptions: [org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed.]\n",
      "23/08/31 16:04:39 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-hbase.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.format(\"hudi\"). \\\n",
    "    options(**hudi_options). \\\n",
    "    mode(\"overwrite\"). \\\n",
    "    save(basePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294c99bf-c499-410c-adad-773dcec7c833",
   "metadata": {},
   "source": [
    "## Querying Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b645f3d6-a9f9-4985-8d41-93431c1393e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tripsSnapshotDF = spark. \\\n",
    "  read. \\\n",
    "  format(\"hudi\"). \\\n",
    "  load(basePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2a889ee-9dee-4424-b9cb-7a4eadad93de",
   "metadata": {},
   "outputs": [],
   "source": [
    "tripsSnapshotDF.createOrReplaceTempView(\"hudi_trips_snapshot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adc49239-f8f1-4bd6-8460-77edf0131205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+-------------------+-------------+\n",
      "|              fare|          begin_lon|          begin_lat|           ts|\n",
      "+------------------+-------------------+-------------------+-------------+\n",
      "|  43.4923811219014| 0.8779402295427752| 0.6100070562136587|1693211548369|\n",
      "|34.158284716382845|0.46157858450465483| 0.4726905879569653|1693273667487|\n",
      "| 66.62084366450246|0.03844104444445928| 0.0750588760043035|1693002314088|\n",
      "| 41.06290929046368| 0.8192868687714224|  0.651058505660742|1693221471879|\n",
      "| 93.56018115236618|0.14285051259466197|0.21624150367601136|1693443534075|\n",
      "| 33.92216483948643| 0.9694586417848392| 0.1856488085068272|1693381867365|\n",
      "| 27.79478688582596| 0.6273212202489661|0.11488393157088261|1692984205507|\n",
      "| 64.27696295884016| 0.4923479652912024| 0.5731835407930634|1693255851431|\n",
      "+------------------+-------------------+-------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select fare, begin_lon, begin_lat, ts from  hudi_trips_snapshot where fare > 20.0\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ff70bb1-450a-4eba-ab46-b152cd13f46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+----------------------+---------+----------+------------------+\n",
      "|_hoodie_commit_time|  _hoodie_record_key|_hoodie_partition_path|    rider|    driver|              fare|\n",
      "+-------------------+--------------------+----------------------+---------+----------+------------------+\n",
      "|  20230831160421605|0247e363-2171-48a...|                      |rider-213|driver-213|  43.4923811219014|\n",
      "|  20230831160421605|67fe72df-2f94-433...|                      |rider-213|driver-213|34.158284716382845|\n",
      "|  20230831160421605|0d4253a0-ed77-4f3...|                      |rider-213|driver-213|17.851135255091155|\n",
      "|  20230831160421605|b75a89e8-7957-4e1...|                      |rider-213|driver-213| 66.62084366450246|\n",
      "|  20230831160421605|c91287b6-9617-4ba...|                      |rider-213|driver-213| 41.06290929046368|\n",
      "|  20230831160421605|e6df7cf1-06af-492...|                      |rider-213|driver-213|19.179139106643607|\n",
      "|  20230831160421605|142f467c-afcb-458...|                      |rider-213|driver-213| 93.56018115236618|\n",
      "|  20230831160421605|df1c451b-9d63-4ed...|                      |rider-213|driver-213| 33.92216483948643|\n",
      "|  20230831160421605|fb529b9d-f68d-494...|                      |rider-213|driver-213| 27.79478688582596|\n",
      "|  20230831160421605|391bcb78-2eea-48f...|                      |rider-213|driver-213| 64.27696295884016|\n",
      "+-------------------+--------------------+----------------------+---------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select _hoodie_commit_time, _hoodie_record_key, _hoodie_partition_path, rider, driver, fare from  hudi_trips_snapshot\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
